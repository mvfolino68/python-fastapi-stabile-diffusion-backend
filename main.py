import base64
import os
from io import BytesIO
from pydantic import BaseModel


import torch
import uvicorn
from diffusers import StableDiffusionPipeline
from dotenv import load_dotenv
from fastapi import FastAPI, Request, Response, status
from fastapi.middleware.cors import CORSMiddleware
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.errors import RateLimitExceeded
from slowapi.util import get_remote_address

load_dotenv()

limiter = Limiter(key_func=get_remote_address, default_limits=["2/minutes"])

app = FastAPI()
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

app.add_middleware(
    CORSMiddleware,
    allow_credentials=True,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_API")
model_id = "runwayml/stable-diffusion-v1-5"
device = "cpu"
pipe = StableDiffusionPipeline.from_pretrained(
    model_id,
    # revision="fp16",
    # torch_dtype=torch.float16,
    use_auth_token=HUGGINGFACE_TOKEN,
    custom_pipeline="lpw_stable_diffusion",
)

if torch.cuda.is_available():
    device = "cuda"
    pipe = pipe.to("device")
    pipe.enable_xformers_memory_efficient_attention()

class PipelineData(BaseModel):
    """ Data class for the pipeline """
    prompt: str
    num_inference_steps: int
    negative_prompt: str = None
    
@app.get(
    "/image",
    # Set what the media type will be in the autogenerated OpenAPI specification.
    # fastapi.tiangolo.com/advanced/additional-responses/#additional-media-types-for-the-main-response
    responses={200: {"content": {"image/png": {}}}},
    # Prevent FastAPI from adding "application/json" as an additional
    # response media type in the autogenerated OpenAPI specification.
    # https://github.com/tiangolo/fastapi/issues/3258
    response_class=Response,
)
@limiter.limit(limit_value="2/5minutes")
def generate(pipeline_data: PipelineData, request: Request):
    prompt = pipeline_data.prompt
    num_inference_steps = pipeline_data.num_inference_steps
    negative_prompt = pipeline_data.negative_prompt
    image = pipe(
        prompt,
        negative_prompt=negative_prompt,
        num_inference_steps=int(num_inference_steps),
        guidance_scale=8.5,
    ).images[0]
    image.save("testimage.png")
    buffer = BytesIO()
    image.save(buffer, format="PNG")
    imgstr = base64.b64encode(buffer.getvalue())

    return Response(content=imgstr, media_type="image/png")


if __name__ == "__main__":
    uvicorn.run("main:app", port=8000, log_level="info")
